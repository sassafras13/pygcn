{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10995482-b1cb-41f9-83cc-42f1cc5a01c5",
   "metadata": {},
   "source": [
    "# Understanding the VGAE and GAE Models\n",
    "\n",
    "The code from pygcn is designed for a simple GCN network so the model architecture is simpler than that in vgae-pytorch, which is designed for a (V)AE. We'll compare the two code bases and understand the VAE and AE architecture in the context of the math. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b1207-eb48-4ac8-9e60-50ff090af2e1",
   "metadata": {},
   "source": [
    "## pygcn Implementation (GCN only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f6f4620-86c9-4e60-8e5a-96fdc072fe7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygcn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4dbfbacda801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpygcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphConvolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygcn'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pygcn.layers import GraphConvolution\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj)) # notice that the ReLU is applied outside of the GCN layer definition\n",
    "        x = F.dropout(x, self.dropout, training=self.training) # does vgae-pytorch use dropout? \n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1) # note that the final output is using log-softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca5c7c-d63d-4642-b76c-8a685e47ae82",
   "metadata": {},
   "source": [
    "## vgae-pytorch Implementation \n",
    "\n",
    "### VGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f96da11f-69df-4a08-a8f7-f86d64911ef5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-adee2cf38074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVGAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'args'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import args\n",
    "\n",
    "class VGAE(nn.Module):\n",
    "\tdef __init__(self, adj):\n",
    "\t\tsuper(VGAE,self).__init__()\n",
    "\t\tself.base_gcn = GraphConvSparse(args.input_dim, args.hidden1_dim, adj) # first layer\n",
    "        # the second layer is generated twice, once for the mean and once for the log-std dev\n",
    "        # this correlates with the original VGAE paper which says that the mean and std dev GCN layers share W0 parameters\n",
    "\t\tself.gcn_mean = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x:x) \n",
    "\t\tself.gcn_logstddev = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x:x)\n",
    "\n",
    "\tdef encode(self, X):\n",
    "\t\thidden = self.base_gcn(X)\n",
    "\t\tself.mean = self.gcn_mean(hidden)\n",
    "\t\tself.logstd = self.gcn_logstddev(hidden)\n",
    "        # the encoder adds N x Nlatent size Gaussian noise to Z during sampling of the latent space\n",
    "\t\tgaussian_noise = torch.randn(X.size(0), args.hidden2_dim)\n",
    "\t\tsampled_z = gaussian_noise*torch.exp(self.logstd) + self.mean\n",
    "\t\treturn sampled_z\n",
    "\n",
    "\tdef forward(self, X):\n",
    "        # the full run of the VGAE is to encode a sample, \n",
    "        # pull a random sample that is not exactly equal to the encoding of the input \n",
    "        # and return the dot product of Z (the random sample) as the prediction Ahat\n",
    "\t\tZ = self.encode(X)\n",
    "\t\tA_pred = dot_product_decode(Z)\n",
    "\t\treturn A_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f217f-e4bd-40c2-87b0-a64c589d81b2",
   "metadata": {},
   "source": [
    "### Explanation of Decoder\n",
    "\n",
    "According to Kipf and Welling, the generative part of the model (i.e. the decoder) can be written as follows: \n",
    "\n",
    "$$p(\\mathbf{\\hat{A}} | \\mathbf{Z}) = \\Pi_{i=1}^N \\Pi_{j=1}^N p(\\hat{A}_{ij} | \\mathbf{z}_i, \\mathbf{z}_j)$$\n",
    "\n",
    "Where the probability distribution can be written as: \n",
    "\n",
    "$$p(\\hat{A}_{ij} | \\mathbf{z}_i, \\mathbf{z}_j) = \\sigma(\\mathbf{z}_i^T \\mathbf{z}_j)$$\n",
    "\n",
    "Where $\\sigma(\\cdot)$ is the logistic sigmoid function:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "We use the logistic sigmoid function because we're using binary cross entropy on the loss function and we want to keep the range of output values of $\\hat{\\mathbf{A}}$ to between 0 and 1. We also use sigmoid activation when we are doing KL divergence...what exactly is the reason, then?\n",
    "\n",
    "NOTE: I do not really understand why this works? Is it because the GCN layers will really learn to generate a latent space that can produce representations which can be multiplied in this way to return a good prediction? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77dc5296-9db3-4de1-bce9-579bdfc15751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_decode(Z):\n",
    "    # the prediction itself is the sigmoid of the product of Z*Z.T \n",
    "\tA_pred = torch.sigmoid(torch.matmul(Z,Z.t()))\n",
    "\treturn A_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdc7c9-0c20-45ba-85ab-01ed8c8e43cb",
   "metadata": {},
   "source": [
    "### GAE \n",
    "\n",
    "This model just removes any stochasticity and represents everything deterministically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0bb759-2bd0-4ea0-82d5-fc75ef991ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAE(nn.Module):\n",
    "\tdef __init__(self,adj):\n",
    "\t\tsuper(GAE,self).__init__()\n",
    "\t\tself.base_gcn = GraphConvSparse(args.input_dim, args.hidden1_dim, adj)\n",
    "        # the second layer just represents the mean of the latent space\n",
    "\t\tself.gcn_mean = GraphConvSparse(args.hidden1_dim, args.hidden2_dim, adj, activation=lambda x:x)\n",
    "\n",
    "\tdef encode(self, X):\n",
    "\t\thidden = self.base_gcn(X)\n",
    "\t\tz = self.mean = self.gcn_mean(hidden)\n",
    "\t\treturn z\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tZ = self.encode(X)\n",
    "\t\tA_pred = dot_product_decode(Z) # decoding is the same as for the VAE\n",
    "\t\treturn A_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b575bc-1c0a-40ce-881d-c7d80f0b865a",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Now let's talk about the loss function used to train the (V)AE. According to Kipf and Welling again, the VAE loss function is the ELBO: \n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{q(\\mathbf{Z}|\\mathbf{X,A})} [\\log p(\\mathbf{A}|\\mathbf{Z})] - \\text{KL}[q(\\mathbf{Z}|\\mathbf{X,A} || p(\\mathbf{Z})]$$\n",
    "\n",
    "Here $\\text{KL}[q(\\cdot) || p(\\cdot)]$ is the Kullback-Leibler divergence between $q(\\cdot)$ and $p(\\cdot)$. \n",
    "\n",
    "The paper says it also uses a Gaussian prior for $p(\\mathbf{Z})$:\n",
    "$$\\begin{align} p(\\mathbf{Z}) &= \\Pi_i p(\\mathbf{z}_i) \\\\\n",
    "&= \\Pi_i \\mathcal{N}(\\mathbf{z}_i | 0, \\mathbf{I})\\end{align}$$\n",
    "\n",
    "After reviewing the code below, I believe we are using a very specific method of computing the KL divergence that formulates it as the \"relative entropy between a diagonal multivariate normal (i.e. $q(\\mathbf{Z}|\\mathbf{X,A}$)( and a standard normal distribution (i.e. $p(\\mathbf{Z})$)\" (source: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence):\n",
    "\n",
    "$$\\begin{align}D_{KL}\\bigg( \\mathcal{N}\\big((\\mu_1, ... , \\mu_k)^T, \\text{diag}(\\sigma_1^2, ..., \\sigma_k^2) \\big) || \\mathcal{N}(0, \\mathbf{I}) \\bigg) &= \\frac{1}{2} \\sum_{i=1}^k (- 1 - \\ln(\\sigma_i^2) + \\sigma_i^2 + \\mu_i^2 ) \\\\\n",
    "&= \\frac{1}{2N} \\text{mean}(\\text{sum}(\\big(1 + 2 ln(\\sigma_i) - (e^{ln(\\sigma_i)})^2 - \\mu_i^2 \\big))) \\end{align}$$\n",
    "\n",
    "I think the mean might be redundant here. Note that the middle two terms are equivalent in the two expressions, we just write the second row a specific way because the log of the standard deviation has been computed as one monolithic term by the second layer of the GCN.\n",
    "\n",
    "It also says that if $\\tilde{\\mathbf{A}}$ is very sparse, then it can help to reweight terms with $\\tilde{\\mathbf{A}}_{ij}=1$ in the loss function, $\\mathcal{L}$. (Although I cannot see where we do that in the code?)\n",
    "\n",
    "Finally, training is done with full-batch gradient descent, and the reparameterization trick (see here: https://sassafras13.github.io/ReparamTrick/) is used to handle the stochasticity of the VAE (not necessary for the AE). \n",
    "\n",
    "### What about the AE? \n",
    "\n",
    "The AE is deterministic so instead of using the KL divergence for the loss function, we use binary cross entropy, aka the log-loss function (see here: https://sassafras13.github.io/BiCE/). This is essentially doing the same thing as KL divergence - it is measuring the difference between two distributions:\n",
    "\n",
    "$$H_p(q) = - \\frac{1}{N} \\sum_{i=1}^N y_i \\log(p(y_i)) + (1 - y_i)\\log(1 - p(y_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d9615-f111-4d18-a716-c39e856bc5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "\n",
    "\n",
    "adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].T), \n",
    "                            torch.FloatTensor(adj_norm[1]), \n",
    "                            torch.Size(adj_norm[2]))\n",
    "adj_label = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].T), \n",
    "                            torch.FloatTensor(adj_label[1]), \n",
    "                            torch.Size(adj_label[2]))\n",
    "features = torch.sparse.FloatTensor(torch.LongTensor(features[0].T), \n",
    "                            torch.FloatTensor(features[1]), \n",
    "                            torch.Size(features[2]))\n",
    "\n",
    "weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0)) \n",
    "weight_tensor[weight_mask] = pos_weight\n",
    "\n",
    "# init model and optimizer\n",
    "model = getattr(model,args.model)(adj_norm)\n",
    "optimizer = Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# train model\n",
    "for epoch in range(args.num_epoch):\n",
    "    t = time.time()\n",
    "\n",
    "    A_pred = model(features)\n",
    "    optimizer.zero_grad()\n",
    "    # the AE uses binary cross entropy\n",
    "    loss = log_lik = norm*F.binary_cross_entropy(A_pred.view(-1), adj_label.to_dense().view(-1), weight = weight_tensor)\n",
    "    if args.model == 'VGAE':\n",
    "        # I think this is the expression for KL divergence for multivariate normal distributions that have a diagonal multivariate normal\n",
    "        kl_divergence = 0.5/ A_pred.size(0) * (1 + 2*model.logstd - model.mean**2 - torch.exp(model.logstd)**2).sum(1).mean() # is this mean redundant?\n",
    "        loss -= kl_divergence\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = get_acc(A_pred,adj_label)\n",
    "\n",
    "    val_roc, val_ap = get_scores(val_edges, val_edges_false, A_pred)\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(loss.item()),\n",
    "          \"train_acc=\", \"{:.5f}\".format(train_acc), \"val_roc=\", \"{:.5f}\".format(val_roc),\n",
    "          \"val_ap=\", \"{:.5f}\".format(val_ap),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "\n",
    "test_roc, test_ap = get_scores(test_edges, test_edges_false, A_pred)\n",
    "print(\"End of training!\", \"test_roc=\", \"{:.5f}\".format(test_roc),\n",
    "      \"test_ap=\", \"{:.5f}\".format(test_ap))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:glm_env]",
   "language": "python",
   "name": "conda-env-glm_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
