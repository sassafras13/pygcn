{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de4fc9ec-a0e4-4d69-af96-68475bd9e28d",
   "metadata": {},
   "source": [
    "# Getting the Correct GCN Layers\n",
    "\n",
    "Both the pygcn and gvae repos have implementations of the GCN layer, I want to see if they are both the same and how they work.\n",
    "\n",
    "Update: I think they are both the same. \n",
    "\n",
    "Let's implement this one step at a time and understand the math along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9892d6a-44a2-4601-b8ae-3c33bee81ae7",
   "metadata": {},
   "source": [
    "## THE MATH: Encoder\n",
    "\n",
    "They use a 2-layer GCN. Together the layers are doing probabilistic inference (in the encoder, specifically?): \n",
    "\n",
    "$$q(\\mathbf{Z} | \\mathbf{X, A}) = \\Pi_{i = 1}^{N} q(\\mathbf{z_i} | \\mathbf{X, A})$$\n",
    "\n",
    "Where we define the conditional probability of the stochastic latent variable $\\mathbf{z_i}$ as: \n",
    "\n",
    "$$q(\\mathbf{z_i} | \\mathbf{X, A}) = \\mathcal{N}(\\mathbf{z_i} | \\mathbf{\\mu_i}, \\text{diag}(\\mathbf{\\sigma_i}^2))$$\n",
    "\n",
    "Therefore, the stochastic latent variable is from a normal distribution that is parameterized by $\\mathbf{\\mu_i}$ and $\\text{diag}(\\mathbf{\\sigma_i})$. We can write these explicitly as: \n",
    "\n",
    "The mean is a matrix of mean vectors, $\\mathbf{\\mu_i}$, i.e.: $\\mathbf{\\mu} = \\text{GCN}_{\\mathbf{\\mu}} (\\mathbf{X, A})$   \n",
    "And the log of the standard deviation is: $\\log(\\mathbf{\\sigma}) = \\text{GCN}_{\\mathbf{\\sigma}} (\\mathbf{X, A})$\n",
    "\n",
    "From this we can see that the neural network we are building is learning to represent an appropriate normal distribution $\\mathcal{N}(\\mu, \\sigma)$ that represents the distribution of stochastic latent variables that are embedded equivalents of the graph defined by $\\mathbf{X, A}$. NOTE: If we instead want to implement a deterministic AE, then we can still use the GCN layers, but we simply take the mean of the layers' output and use the log likelihood instead of the KL divergence as our loss function. We can define the neural network, i.e. the GCN layer itself, as follows: \n",
    "\n",
    "$$\\text{GCN}(\\mathbf{X, A}) = \\mathbf{\\tilde{A}} \\text{ReLU} (\\mathbf{\\tilde{A}X W_0}) \\mathbf{W_1}$$\n",
    "\n",
    "where the weight matrices are $\\mathbf{W_0, W_1}$. If this feels like it is coming out of left field, my blog post here explains the derivation of this expression in detail: https://sassafras13.github.io/graphConv/. \n",
    "\n",
    "Both the mean and standard deviation computed by the GCN (i.e. $\\text{GCN}_{\\mu}$ and $\\text{GCN}_{\\sigma}$) share the same first layer parameters/weights $\\mathbf{W_0}$. \n",
    "\n",
    "Recall that we can define $\\text{ReLU}(\\cdot) = \\max(0, \\cdot)$. And $\\mathbf{\\tilde{A}}$ is the normalized adjacency matrix we computed previously. \n",
    "\n",
    "### Matrix Dimensions\n",
    "\n",
    "To better understand the code below, I want to explicitly write out the matrix dimensions. \n",
    "\n",
    "- [ ] N is number of nodes in the graph\n",
    "- [ ] F is number of features for each node in the graph \n",
    "- [ ] Nhid is the number of hidden features between layer 1 and 2 of the encoder  \n",
    "- [ ] Nclass is the number of classes that the GCN is trying to predict - this is the size of the output. For us I think this would be Nlatent, i.e. how big we want our latent space to be. \n",
    "\n",
    "- [ ] $\\mathbf{\\tilde{A}} = N \\times N$\n",
    "- [ ] $\\mathbf{X} = N \\times F$   \n",
    "- [ ] $\\mathbf{W_0} = F \\times Nhid$  \n",
    "- [ ] $\\mathbf{W_1} = Nhid \\times Nlatent$\n",
    "- [ ] $\\mathbf{Z} = N \\times Nlatent$\n",
    "\n",
    "So all together: \n",
    "\n",
    "$$\\begin{aligned} \\mathbf{\\tilde{A}} \\text{ReLU} (\\mathbf{\\tilde{A}X W_0}) \\mathbf{W_1} &= (N \\times N) \\times \\bigg( (N \\times N) \\times (N \\times F) \\times (F \\times \\text{Nhid}) \\bigg) \\times (\\text{Nhid} \\times \\text{Nlatent}) \\\\ \n",
    "&= (N \\times N) \\times \\bigg( (N \\times \\text{Nhid}) \\bigg) \\times (\\text{Nhid} \\times \\text{Nlatent}) \\\\\n",
    "&= (N  \\times \\text{Nlatent}) \\\\\n",
    "&= \\mathbf{Z} \\end{aligned}$$\n",
    "\n",
    "Now that we have understood this part, let us go to the code to see how this GCN is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1588aad8-7ff1-4a61-9aeb-5487ec8e0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter # this is a kind of tensor that is automatically considered as the parameter of a class/module\n",
    "from torch.nn.modules.module import Module # torch base class for all NN modules, always inherit from here\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        # according to: https://discuss.pytorch.org/t/super-model-in-init/97426\n",
    "        # The super call delegates the function call to the parent class, \n",
    "        # which is nn.Module in your case. \n",
    "        # This is needed to initialize the nn.Module properly. \n",
    "        # from: https://realpython.com/python-super/\n",
    "        # high level super() gives you access to methods in a superclass \n",
    "        # from the subclass that inherits from it\n",
    "        # and from: https://medium.com/@ariellemesser/pytorch-nn-module-super-classes-sub-classes-inheritance-and-call-speci-3cc277407ff5\n",
    "        # In the super class, nn.Module, there is a __call__ method which \n",
    "        # obtains the forward function from the subclass and calls it.\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features)) # why wouldn't you have bias?\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # we define the reset_parameters method\n",
    "    def reset_parameters(self):\n",
    "        # why are we using the size of the weight matrix to compute std dev? \n",
    "        # self.weight.size(1) = Nhid for W0, Nlatent for W1\n",
    "        # this expression is essentially assuming that the squared residual between the data and \n",
    "        # the mean is 1, and the number of data points in the sample is Nhid or Nlatent\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1)) \n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # together, support and output are computing\n",
    "        # A_tilde * X * W by doing\n",
    "        # support = X * W\n",
    "        # output = A_tilde * support = A_tilde * X * W\n",
    "        support = torch.mm(input, self.weight) # mat mul\n",
    "        output = torch.spmm(adj, support) # sparse mat mul\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf75a1-2abb-4074-b618-45f998b12009",
   "metadata": {},
   "source": [
    "## Compare to VGAE Repo\n",
    "\n",
    "In comparing this definition of the GCN layer to the VGAE repo, we have pretty much the same thing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aed366e7-2e06-4f07-a9f7-4f459c16c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcc43d0d-9e26-4ceb-8ad8-198cd1ed4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvSparse(nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim, adj, activation = F.relu, **kwargs):\n",
    "\t\tsuper(GraphConvSparse, self).__init__(**kwargs)\n",
    "\t\tself.weight = glorot_init(input_dim, output_dim) \n",
    "\t\tself.adj = adj\n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, inputs):\n",
    "\t\tx = inputs\n",
    "\t\tx = torch.mm(x,self.weight)\n",
    "\t\tx = torch.mm(self.adj, x)\n",
    "\t\toutputs = self.activation(x)\n",
    "\t\treturn outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d41174e-0228-4ac4-9994-84e0e024853d",
   "metadata": {},
   "source": [
    "The primary differences here are that:\n",
    "- This implementation incorporates the activation function in the layer definition (the implementation above does it at the model level)  \n",
    "- This implementation initializes the weights using the function glorot_init which is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9bf78a9-bfa3-4b8a-9e5c-99a5555496c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_init(input_dim, output_dim):\n",
    "\tinit_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "\tinitial = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "\treturn nn.Parameter(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d111837b-c025-4fa1-85b0-78e87123eefa",
   "metadata": {},
   "source": [
    "Ref: https://jamesmccaffrey.wordpress.com/2017/06/21/neural-network-glorot-initialization/\n",
    "\n",
    "The Glorot initialization method (also referred to as Xavier initialization) was proposed by Glorot and Bengio in 2010 (https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). It is optimized for deep NNs that use ReLU activation. In normal weight activation, we set the weights and biases to uniform random values in the range [-0.01, +0.01]. But this does not work as well for deep NNs with ReLU activation. In this case, we use Glorot initialization which intializes each weight with a small Gaussian (or uniform) value with mean 0.0 and variance determined by the fan-in and fan-out of the weights. \n",
    "\n",
    "Fan-in and fan-out refer to the number of input nodes and output nodes/hidden nodes that the weights map to. If you are using the Gaussian distribution with Glorot, then you would compute: \n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{2}{\\text{fan-in} + \\text{fan-out}}}$$\n",
    "$$\\text{weight} = \\mathcal{N}(\\mu = 0.0, \\sigma)$$\n",
    "\n",
    "If instead you were doing Glorot initialization with Uniform distribution (as we are above) then you would compute: \n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{6}{\\text{fan-in} + \\text{fan-out}}}$$\n",
    "$$\\text{weight} = \\mathcal{N}(\\mu = 0.0, \\sigma)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:glm_env]",
   "language": "python",
   "name": "conda-env-glm_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
